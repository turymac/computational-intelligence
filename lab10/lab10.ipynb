{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import namedtuple, defaultdict\n",
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = namedtuple('Position', ['x', 'o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGIC = [2, 7, 6,\n",
    "         9, 5, 1,\n",
    "         4, 3, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_board(pos):\n",
    "    for r in range(3):\n",
    "        for c in range(3):\n",
    "            i = r * 3 + c\n",
    "            if MAGIC[i] in pos.x:\n",
    "                print('X', end=' ')\n",
    "            elif MAGIC[i] in pos.o:\n",
    "                print('O', end=' ')\n",
    "            else:\n",
    "                print(MAGIC[i], end=' ')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def win(elements):\n",
    "    return any(sum(c) == 15 for c in combinations(elements,3))\n",
    "\n",
    "def state_value(pos: State):\n",
    "    if win(pos.x):\n",
    "        return 1\n",
    "    elif win(pos.o):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_states():\n",
    "    states = set()\n",
    "    generate(0,State(set(),set()),states)\n",
    "    generate(1,State(set(),set()),states)\n",
    "    return states\n",
    "\n",
    "def generate(player,state,discovered):\n",
    "    if(len(state.x) + len(state.o) == 0):\n",
    "        discovered.add((State(frozenset(), frozenset()),player))\n",
    "    if(win(state.x)):\n",
    "        return\n",
    "    if(win(state.o)):\n",
    "        return\n",
    "    available = set(range(1,9+1)) - state.x - state.o\n",
    "    if available:\n",
    "        for move in list(available):\n",
    "            tmp = State(set(state.x),set(state.o))\n",
    "            tmp[player].add(move)\n",
    "            discovered.add((State(frozenset(tmp.x),frozenset(tmp.o)),1-player))\n",
    "            generate(1-player,tmp,discovered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_states = generate_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_dictionary = defaultdict(float)\n",
    "policy_dict = defaultdict(int)\n",
    "\n",
    "for state, to_move in possible_states:\n",
    "    available = list(set(range(1, 9+1)) - state.x - state.o)\n",
    "    if available and not to_move:   #Mapping required only for states in which WE are to move\n",
    "        move = choice(available)    #Building a first random policy\n",
    "        policy_dict[state] = move   #(Key,Value) = (State,Action_to_perform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(state):\n",
    "    return state_value(state)\n",
    "\n",
    "def possible_actions(state):\n",
    "    availables = list(set(range(1, 9+1)) - state.x - state.o)\n",
    "    possibles = []\n",
    "    for i in availables:\n",
    "        tmp = set(state.x)\n",
    "        tmp.add(i)\n",
    "        possibles.append((i, State(frozenset(tmp),frozenset(state.o)))) #(Key,Value) = (Possible_action,Resultant_state)\n",
    "    return possibles    #All possible existing actions starting from state \n",
    "\n",
    "def model_next_state(state):    #A reasonable model of our opponent\n",
    "    availables = list(set(range(1, 9+1)) - state.x - state.o)\n",
    "    \n",
    "    for c in combinations(state.o,2):   #If he can win with a move he goes for it\n",
    "        for el in availables:\n",
    "            if sum(c) + el == 15:\n",
    "                tmp = set(state.o)\n",
    "                tmp.add(el)\n",
    "                return [State(frozenset(state.x),frozenset(tmp))] \n",
    "    \n",
    "    for c in combinations(state.x,2):   #If we are going to win in a move he stops us\n",
    "        for el in availables:\n",
    "            if sum(c) + el == 15:\n",
    "                tmp = set(state.o)\n",
    "                tmp.add(el)\n",
    "                return [State(frozenset(state.x),frozenset(tmp))]\n",
    "    \n",
    "    possibles = []\n",
    "    \n",
    "    for i in availables:    #Otherwise he goes random \n",
    "        tmp = set(state.o)\n",
    "        tmp.add(i)\n",
    "        possibles.append(State(frozenset(state.x),frozenset(tmp)))\n",
    "    return possibles\n",
    "\n",
    "def apply_action(state,action):\n",
    "    tmp = set(state.x)\n",
    "    tmp.add(action)\n",
    "    return State(frozenset(tmp),frozenset(state.o))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "teta = 0.001    #parameter to tweak termination of policy_evaluation function\n",
    "dr = .1     #experiments show that larger values yield not optimal policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(teta, dr):\n",
    "    delta = 1\n",
    "    while delta > teta:\n",
    "        delta = 0\n",
    "        for state in list(policy_dict.keys()):\n",
    "            old_value = value_dictionary[state]\n",
    "            action = policy_dict[state]\n",
    "            new_value = 0\n",
    "            candidates = model_next_state(apply_action(state,action))   #Given the action suggested by actual policy compute all opponent possible response\n",
    "            for possible_state in candidates:\n",
    "                new_value += (1/len(candidates)) * (reward(possible_state) + dr * value_dictionary[possible_state]) # V(s) = sum(prob(s',r|s,policy(s)))[reward(s')+dr*V(s')] for all possible s'\n",
    "            value_dictionary[state] = new_value\n",
    "            delta = max(delta, abs(old_value - value_dictionary[state]))\n",
    "\n",
    "def policy_improvement(dr):\n",
    "    policy_stable = True\n",
    "    for state in list(policy_dict.keys()):\n",
    "        old_action = policy_dict[state]\n",
    "        max_expected = -10\n",
    "        best_action = old_action\n",
    "        for action, resultant_state in possible_actions(state): #Selecting action with most high expected return \n",
    "            candidates = model_next_state(resultant_state)\n",
    "            expected = 0\n",
    "            for possible_state in candidates:\n",
    "                expected += (1/len(candidates)) * (reward(possible_state) + dr * value_dictionary[possible_state])\n",
    "            if expected > max_expected:\n",
    "                best_action = action\n",
    "                max_expected = expected\n",
    "        policy_dict[state] = best_action    #Updating policy\n",
    "        if old_action != best_action:\n",
    "            policy_stable = False\n",
    "    return policy_stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable = False\n",
    "while not stable:\n",
    "    policy_evaluation(teta,dr)    #Makes the value function consistent with current policy\n",
    "    stable = policy_improvement(dr) #Improve policy with respect to the updaated value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def almost_random_policy(state):    #Our opponent, he plays random unless he can win in a move o we are going to win in a move\n",
    "    available = set(range(1,9+1)) - state.x - state.o\n",
    "    for c in combinations(state.o,2):\n",
    "        for el in available:\n",
    "            if sum(c) + el == 15:\n",
    "                return el\n",
    "    for c in combinations(state.x,2):\n",
    "        for el in available:\n",
    "            if sum(c) + el == 15:\n",
    "                return el\n",
    "    return choice(list(available))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(n_games):\n",
    "    scores = [0,0]\n",
    "    for _ in range(n_games):\n",
    "        state = State(set(), set())\n",
    "        available = set(range(1, 9+1))\n",
    "        pl = choice([0,1])\n",
    "        while available:\n",
    "            if(pl):\n",
    "                ply = almost_random_policy(state)   #Opponent\n",
    "            else:\n",
    "                ply = policy_dict[State(x=frozenset(state.x),o=frozenset(state.o))] #Our agent is following his policy\n",
    "            state[pl].add(ply)\n",
    "            available.remove(ply)\n",
    "            if win(state[pl]):\n",
    "                scores[pl] += 1\n",
    "                break\n",
    "            pl = 1 - pl\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hero won 100.0% of no tie games\n"
     ]
    }
   ],
   "source": [
    "stats = play_game(100000)\n",
    "\n",
    "print(f\"Hero won {(stats[0]/(stats[1]+stats[0]))*100}% of no tie games\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
